{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aca5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b346f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212e4b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad41f187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effe8ef",
   "metadata": {},
   "source": [
    "## Unique characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "842a0a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Unique characters count =  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Unique characters count = ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8138ad",
   "metadata": {},
   "source": [
    "## Tokenize using the char to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d90867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to index mapping  {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "\n",
      "Index to string mapping {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "\n",
      "\n",
      "Encoded string for sample hi there is  [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "Decoded string for sample hi there is  hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "print('String to index mapping ', stoi)\n",
    "print('\\nIndex to string mapping', itos)\n",
    "\n",
    "encode = lambda s:[stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "## sample\n",
    "print('\\n')\n",
    "print('Encoded string for sample hi there is ' , encode('hii there'))\n",
    "print('Decoded string for sample hi there is ' , decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669c7c9",
   "metadata": {},
   "source": [
    "## Tokenize complete the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8048a8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ad4bc",
   "metadata": {},
   "source": [
    "## Split the data into train and test with 90%train and 10%test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "201eb343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data chars  torch.Size([111540])\n",
      "Train data chars  torch.Size([1003854])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data)*0.9)\n",
    "train_data = data[:n]\n",
    "val_data  = data[n:]\n",
    "print('Val data chars ', val_data.shape)\n",
    "print('Train data chars ', train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09d5a9",
   "metadata": {},
   "source": [
    "## Max length and adding max length+1 because max_length+1 is target of max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb6d9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "005d6eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When inputs is tensor([18]) the target : 47\n",
      "When inputs is tensor([18, 47]) the target : 56\n",
      "When inputs is tensor([18, 47, 56]) the target : 57\n",
      "When inputs is tensor([18, 47, 56, 57]) the target : 58\n",
      "When inputs is tensor([18, 47, 56, 57, 58]) the target : 1\n",
      "When inputs is tensor([18, 47, 56, 57, 58,  1]) the target : 15\n",
      "When inputs is tensor([18, 47, 56, 57, 58,  1, 15]) the target : 47\n",
      "When inputs is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target : 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    input_data = x[:i+1]\n",
    "    output_data = y[i]\n",
    "    print(f'When inputs is {input_data} the target : {output_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0469c",
   "metadata": {},
   "source": [
    "## batch generator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d94f7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "1601a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(0, len(data)-block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i: i+block_size ]for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e329e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, targets = batch_generator(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fa5a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "When inputs is tensor([46]) the target : 1\n",
      "When inputs is tensor([46,  1]) the target : 51\n",
      "When inputs is tensor([46,  1, 51]) the target : 43\n",
      "When inputs is tensor([46,  1, 51, 43]) the target : 8\n",
      "When inputs is tensor([46,  1, 51, 43,  8]) the target : 0\n",
      "When inputs is tensor([46,  1, 51, 43,  8,  0]) the target : 0\n",
      "When inputs is tensor([46,  1, 51, 43,  8,  0,  0]) the target : 19\n",
      "When inputs is tensor([46,  1, 51, 43,  8,  0,  0, 19]) the target : 24\n",
      "\n",
      "\n",
      "When inputs is tensor([1]) the target : 39\n",
      "When inputs is tensor([ 1, 39]) the target : 1\n",
      "When inputs is tensor([ 1, 39,  1]) the target : 45\n",
      "When inputs is tensor([ 1, 39,  1, 45]) the target : 43\n",
      "When inputs is tensor([ 1, 39,  1, 45, 43]) the target : 52\n",
      "When inputs is tensor([ 1, 39,  1, 45, 43, 52]) the target : 58\n",
      "When inputs is tensor([ 1, 39,  1, 45, 43, 52, 58]) the target : 50\n",
      "When inputs is tensor([ 1, 39,  1, 45, 43, 52, 58, 50]) the target : 43\n",
      "\n",
      "\n",
      "When inputs is tensor([59]) the target : 58\n",
      "When inputs is tensor([59, 58]) the target : 1\n",
      "When inputs is tensor([59, 58,  1]) the target : 53\n",
      "When inputs is tensor([59, 58,  1, 53]) the target : 52\n",
      "When inputs is tensor([59, 58,  1, 53, 52]) the target : 50\n",
      "When inputs is tensor([59, 58,  1, 53, 52, 50]) the target : 63\n",
      "When inputs is tensor([59, 58,  1, 53, 52, 50, 63]) the target : 1\n",
      "When inputs is tensor([59, 58,  1, 53, 52, 50, 63,  1]) the target : 58\n",
      "\n",
      "\n",
      "When inputs is tensor([21]) the target : 33\n",
      "When inputs is tensor([21, 33]) the target : 31\n",
      "When inputs is tensor([21, 33, 31]) the target : 10\n",
      "When inputs is tensor([21, 33, 31, 10]) the target : 0\n",
      "When inputs is tensor([21, 33, 31, 10,  0]) the target : 32\n",
      "When inputs is tensor([21, 33, 31, 10,  0, 32]) the target : 46\n",
      "When inputs is tensor([21, 33, 31, 10,  0, 32, 46]) the target : 43\n",
      "When inputs is tensor([21, 33, 31, 10,  0, 32, 46, 43]) the target : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(inp.shape, target.shape)\n",
    "for i in range(inp.shape[0]):\n",
    "    for j in range(block_size):\n",
    "        input_data = inp[i][:j+1]\n",
    "        output_data = targets[i][j]\n",
    "        print(f'When inputs is {input_data} the target : {output_data}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7e574",
   "metadata": {},
   "source": [
    "## Implement on Bigram Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ee8a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b41b23be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5572, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "ErQ\n",
      "ymzlViXae:!ByjvAPQ.Xg3eT\n",
      " UpmH.EIqfmLGTCs'fMnvBPypPK!PqQN;AO.j-jDpN.S $;mgQCu$vtaTpgb Nz\n",
      "lV!t-UI\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx and targets are (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #(B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # idx = (1, 1)\n",
    "#             print(logits.shape) # (1, 1, 65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "#             idx_next = torch.argmax(probs) # (B, 1)\n",
    "#             idx_next  = idx_next.view(1, 1)\n",
    "#             print(idx_next.shape, )\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(inp, targets)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d75a9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82772945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.474486827850342\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    xb, yb = batch_generator('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "22ed6b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MPrg r t ngr se ha me IAninoud\n",
      "Tith thas. is y,\n",
      "A:\n",
      "Whil incthery ivistcer.\n",
      "LAuly d trer hederon meff t I inde ILORI frengur ss f the ccheee.\n",
      "be by,\n",
      "\n",
      "Indemy s sou erito ay, s de uran IELonn tho the KAn\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fb57b",
   "metadata": {},
   "source": [
    "## Mathematical trick for self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "568597ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293b23a",
   "metadata": {},
   "source": [
    "#### Inorder to interact between the tokens it needs to communicate between each other and we need to write the script to communicate with the previous tokens because the future tokens will be predicted. For example, for t8 token we will average the tokens till t0 to t8 and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6bfeabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(x_prev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc29bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4123f7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0495, -1.8143],\n",
       "        [ 0.1679,  1.5913],\n",
       "        [-1.9818, -0.5628],\n",
       "        [-0.8134, -2.1756],\n",
       "        [ 0.4517, -0.2635],\n",
       "        [ 0.1707,  0.7125],\n",
       "        [ 2.8604,  1.0229],\n",
       "        [-0.2694,  1.2245]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "58e39f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0495, -1.8143],\n",
       "        [ 0.1087, -0.1115],\n",
       "        [-0.5882, -0.2619],\n",
       "        [-0.6445, -0.7403],\n",
       "        [-0.4252, -0.6450],\n",
       "        [-0.3259, -0.4187],\n",
       "        [ 0.1293, -0.2128],\n",
       "        [ 0.0794, -0.0331]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b0bf2",
   "metadata": {},
   "source": [
    "#### Efficient way to perform this trick is using the matrix multiplication and using tril matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5d01ad62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "411a8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b = \n",
      "tensor([[7., 1., 3.],\n",
      "        [1., 1., 4.],\n",
      "        [4., 6., 9.]])\n",
      "c = \n",
      "tensor([[7.0000, 1.0000, 3.0000],\n",
      "        [4.0000, 1.0000, 3.5000],\n",
      "        [4.0000, 2.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a/torch.sum(a, 1, keepdims=True)\n",
    "print('a = ')\n",
    "print(a)\n",
    "b = torch.randint(0, 10, (3, 3)).float()\n",
    "print('b = ')\n",
    "print(b)\n",
    "c = a@b\n",
    "print('c = ')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "22dc8081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0495, -1.8143],\n",
       "        [ 0.1087, -0.1115],\n",
       "        [-0.5882, -0.2619],\n",
       "        [-0.6445, -0.7403],\n",
       "        [-0.4252, -0.6450],\n",
       "        [-0.3259, -0.4187],\n",
       "        [ 0.1293, -0.2128],\n",
       "        [ 0.0794, -0.0331]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rewriting the xbow script\n",
    "xbow2 = torch.zeros(B, T, C)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei/torch.sum(wei, 1, keepdims=True)\n",
    "xbow2 = wei @ x ## (B, T,T)(broadcasting to B batches) @ (B, T, C) --> (B, T , C)\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "db024ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2) ## comparing previous xbow and latest xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "67bfde52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei = \n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "wei after softmax = \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using softmax \n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "print('wei = ')\n",
    "print(wei)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print('wei after softmax = ')\n",
    "print(wei)\n",
    "xbow3 = wei @ x\n",
    "# print(xbow3[0])\n",
    "torch.allclose(xbow, xbow3) ## comparing previous xbow and latest xbow3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223b014",
   "metadata": {},
   "source": [
    "## Adding the positional encoder to the bi-gram model and predicting for maximum block size itself \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "62a502e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "eecfda9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6901, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "nStZrDUhF&RdVlw?y E!D'vaSVMZ:3a;ml;AJmmvOdCGgBiDmYJ&;bOjLyLJAa.&ow;JAbFWi'yl;;d!SyHz;i;vPvJ$oSpQLcCA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.positional_encoder = nn.Embedding(block_size, n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        #idx and targets are (B, T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) #(B, T, n_emb)\n",
    "        pos_emb = self.positional_encoder(torch.arange(T)) #(T, n_emb)\n",
    "        x = token_emb+pos_emb #(B, T, n_emb)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "#             logits, loss = self(idx)  # idx = (1, 1)\n",
    "            logits, loss = self(idx[:, -block_size:])  ## giving maximum previous block size characters itself\n",
    "#             print(logits.shape) # (1, 1, 65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "logits, loss = model(inp, targets)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ad50c",
   "metadata": {},
   "source": [
    "## Adding Multi head self attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4b4f6b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## self attention each token has query and key where each token query tries to multply with the key \n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "# x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "\n",
    "k = key(x) #(B, T, head_size)\n",
    "q = query(x) #(B, T, head_size)\n",
    " \n",
    "wei = q @ k.transpose(-2, -1) #(B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "# print(wei[0])\n",
    "\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "output = wei @ v\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0dae8c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5771, 0.4229, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5207, 0.3891, 0.0903, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2874, 0.1826, 0.3358, 0.1943, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1798, 0.7538, 0.0209, 0.0213, 0.0242, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0800, 0.0267, 0.0760, 0.1100, 0.4024, 0.3049, 0.0000, 0.0000],\n",
       "        [0.0262, 0.1885, 0.1014, 0.0784, 0.0856, 0.2900, 0.2299, 0.0000],\n",
       "        [0.0638, 0.0546, 0.0880, 0.1961, 0.1057, 0.2379, 0.0953, 0.1585]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wei[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3da598",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "80a39ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d2830c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9416)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5c663998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0104)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4cf44a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0879)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5d115e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0c711f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "33696618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \" one head of self-attention\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(C, head_size, bias=False)\n",
    "        self.query = nn.Linear(C, head_size, bias=False)\n",
    "        self.value = nn.Linear(C, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## (B, T, C) @ (B, C, T) --> (B, T, T)\n",
    "        \n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e56ab1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.3215, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "d3BR'o&lGnPrLK3bhf$v\n",
      "WFSggLRLHObfr?G.E hwNrmGHX!WO,VwAMJJjtaNfwTo$xf$hh:IAZBOHChJOlm b!-iZL-yxU?P\n",
      "mh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.positional_encoder = nn.Embedding(block_size, n_emb)\n",
    "        self.head = Head(n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        #idx and targets are (B, T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) #(B, T, n_emb)\n",
    "        pos_emb = self.positional_encoder(torch.arange(T)) #(T, n_emb)\n",
    "        x = token_emb+pos_emb #(B, T, n_emb)\n",
    "        x = self.head(x) #(B, T, n_emb)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "#             logits, loss = self(idx)  # idx = (1, 1)\n",
    "            logits, loss = self(idx[:, -block_size:])  ## giving maximum previous block size characters itself\n",
    "#             print(logits.shape) # (1, 1, 65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "logits, loss = model(inp, targets)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a2f19708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.mhsa_blocks = nn.ModuleList([Head(head_size) for n in range(num_heads)])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.mhsa_blocks], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "18efcee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.1563, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "ivmnUwScdkPGczaZ.u3u ,eBBtFl Wp,.QgwbcYRGUhTJz3cu-o3Nv?E!XG:j!ZDW.ajRqekQSXs.X;X$h?tkx&aDs&GTpPaC\n",
      "An\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.positional_encoder = nn.Embedding(block_size, n_emb)\n",
    "        self.head = MultiHeadSelfAttention(4, n_emb//4) # i.e., 4 heads of 8 dimensional self attention \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        #idx and targets are (B, T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) #(B, T, n_emb)\n",
    "        pos_emb = self.positional_encoder(torch.arange(T)) #(T, n_emb)\n",
    "        x = token_emb+pos_emb #(B, T, n_emb)\n",
    "        x = self.head(x) #(B, T, n_emb)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "#             logits, loss = self(idx)  # idx = (1, 1)\n",
    "            logits, loss = self(idx[:, -block_size:])  ## giving maximum previous block size characters itself\n",
    "#             print(logits.shape) # (1, 1, 65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "logits, loss = model(inp, targets)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "ec326637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.033801555633545\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    xb, yb = batch_generator('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a76a7f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cat wor song cove my beetster:\n",
      "Bans Yould.\n",
      "\n",
      "RULIUS: Eing par sponomins is ide, blabe fories?\n",
      "\n",
      "Cot.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49235811",
   "metadata": {},
   "source": [
    "### Adding MLP with multi blocks of MHSA and add projection layer along with the skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4308fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), ## projection layer\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2d9c3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.mhsa_blocks = nn.ModuleList([Head(head_size) for n in range(num_heads)])\n",
    "        self.projection_layer = nn.Linear(n_emb, n_emb)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.mhsa_blocks], dim=-1)\n",
    "        out = self.projection_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d7b7385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadSelfAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_emb)\n",
    "    def forward(self, x):\n",
    "        x = x+self.sa(x) ## skip connection\n",
    "        x = x+self.ffwd(x) # skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "c00f0780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.4380, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "HEkkP!u'XNV,:hOcd$&RENJa? Z&esjjUVv;&$Rc$$;&,o'ADUT&eqqtb,&'$3C?ojfAfeNOz&RNRazB.n$&.,aO$&CS!s?Tsa&W\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.positional_encoder = nn.Embedding(block_size, n_emb)\n",
    "        self.block = nn.Sequential(Block(n_emb, 4), Block(n_emb, 4), Block(n_emb, 4))\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        #idx and targets are (B, T) tensor of integers\n",
    "        token_emb = self.token_embedding_table(idx) #(B, T, n_emb)\n",
    "        pos_emb = self.positional_encoder(torch.arange(T)) #(T, n_emb)\n",
    "        x = token_emb+pos_emb #(B, T, n_emb)\n",
    "        x = self.block(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "            \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "#             logits, loss = self(idx)  # idx = (1, 1)\n",
    "            logits, loss = self(idx[:, -block_size:])  ## giving maximum previous block size characters itself\n",
    "#             print(logits.shape) # (1, 1, 65)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "logits, loss = model(inp, targets)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "90aa74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9b01b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0799484252929688\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    xb, yb = batch_generator('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "cc2fbc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O len\n",
      "CARWICUS:\n",
      "My wo we:\n",
      "Thou brem good bill of 'twent once, is I saway,\n",
      "Kentle? \n",
      "LY VOLIXENBRYORD \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a0a20",
   "metadata": {},
   "source": [
    "## Adding layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "af7e9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizes mean = 0 and stddev = 1 for every batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "9a827748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.randn(32, 100)\n",
    "layer_norm = nn.LayerNorm(100)\n",
    "p1 = layer_norm(p)\n",
    "p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "be94950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1147, grad_fn=<MeanBackward0>),\n",
       " tensor(0.9386, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1[:,0].mean(), p1[:,0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b3018e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4305e-08, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0050, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1[0, :].mean(), p1[0, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadSelfAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_emb)\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "    def forward(self, x):\n",
    "        x = x+self.sa(self.ln1(x)) ## skip connection\n",
    "        x = x+self.ffwd(self.ln1(x)) # skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea21bc3c",
   "metadata": {},
   "source": [
    "## Complete updated code with adding few dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "29b5b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "block_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fe80acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.2443, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Fh!tLuPesSCCTy!$H,s-3M$bYi?&?!qv$W!?Rg;I,qG,yF3ieSIba-?S'-.&,r!:-YA-lfob.WTq?GS?DGb\n",
      "R,S?Y?iyYZiELeKK\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "logits, loss = model(inp, targets)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c4bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "fbf54210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3831497430801392\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "# batch_size = 32\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    xb, yb = batch_generator('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a60311ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a altes it jurth, tefore and to see,\n",
      "And fear at heaven to adminon brother:\n",
      "As time madam, and commortains poor minds.\n",
      "\n",
      "KING LEWARD IV:\n",
      "I'll spoke mouch them?\n",
      "Will is thn I say I claid 'twast crave to Rome.\n",
      "\n",
      "YORK:\n",
      "Norfoline emband Deat Marcius Rick? we prope, our stay\n",
      "and wills, that could be seem thee, with suspervant,\n",
      "Tis find freed withoutlessome whave, the worst\n",
      "That he this. GoO me, sir, becaused it is, in hard;\n",
      "Yea seal to clies, titlemany: mone thou kAjoints\n",
      "The doth man's swater is the d\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24327e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
